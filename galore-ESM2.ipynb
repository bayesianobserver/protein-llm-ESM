{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32635630-3825-4346-9017-a034e0d6eefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "2024-04-19 16:10:20.364962: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-19 16:10:21.611944: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-19 16:10:21.611975: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-04-19 16:10:24.610872: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-19 16:10:24.611040: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-19 16:10:24.611057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "import evaluate \n",
    "import datasets\n",
    "import requests\n",
    "import pandas\n",
    "import sklearn\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106fd271-cd4e-4a3d-ba15-87ba18c57da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.39.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf170f0-2d4f-4168-8f60-cfe65ab2aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'facebook/esm2_t6_8M_UR50D' # This is the smallest of the ESM2 models: 6 layers, 8M params. \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107f380-9bc4-46cb-be67-235f359aff78",
   "metadata": {},
   "source": [
    "Let's download some data for a protein binary classification problem. In this case, we will attempt to predict whether a protein lives iinside a cell or on its membrane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a19d8d-1ce0-4de4-b907-7b5495b986e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7258/1882454772.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cytosolic_df['label'] = 0\n",
      "/tmp/ipykernel_7258/1882454772.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  membrane_df['label'] = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>ind</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>Q14714</td>\n",
       "      <td>MGKNKQPRGQQRQGGPPAADAAGPDDMEPKKGTGAPKECGEEEPRT...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cell membrane {ECO:00002...</td>\n",
       "      <td>243</td>\n",
       "      <td>3619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>O95971</td>\n",
       "      <td>MLLEPGRGCCALAILLAIVDIQSGGCINITSSASQEGTRLNLICTV...</td>\n",
       "      <td>SUBCELLULAR LOCATION: [CD160 antigen]: Cell me...</td>\n",
       "      <td>181</td>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>Q8TDB4</td>\n",
       "      <td>MYLRRAVSKTLALPLRAPPNPAPLGKDASLRRMSSNRFPGSSGSNM...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Mitochondrion {ECO:00002...</td>\n",
       "      <td>240</td>\n",
       "      <td>5146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11492</th>\n",
       "      <td>Q8ND94</td>\n",
       "      <td>MLGSPCLLWLLAVTFLVPRAQPLAPQDFEEEEADETETAWPPLPAV...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "      <td>238</td>\n",
       "      <td>11492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>P14550</td>\n",
       "      <td>MAASCVLLHTGQKMPLIGLGTWKSEPGQVKAAVKYALSVGYRHIDC...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytosol {ECO:...</td>\n",
       "      <td>325</td>\n",
       "      <td>1594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry                                           Sequence  \\\n",
       "3619   Q14714  MGKNKQPRGQQRQGGPPAADAAGPDDMEPKKGTGAPKECGEEEPRT...   \n",
       "907    O95971  MLLEPGRGCCALAILLAIVDIQSGGCINITSSASQEGTRLNLICTV...   \n",
       "5146   Q8TDB4  MYLRRAVSKTLALPLRAPPNPAPLGKDASLRRMSSNRFPGSSGSNM...   \n",
       "11492  Q8ND94  MLGSPCLLWLLAVTFLVPRAQPLAPQDFEEEEADETETAWPPLPAV...   \n",
       "1594   P14550  MAASCVLLHTGQKMPLIGLGTWKSEPGQVKAAVKYALSVGYRHIDC...   \n",
       "\n",
       "                               Subcellular location [CC]  seq_len    ind  \\\n",
       "3619   SUBCELLULAR LOCATION: Cell membrane {ECO:00002...      243   3619   \n",
       "907    SUBCELLULAR LOCATION: [CD160 antigen]: Cell me...      181    907   \n",
       "5146   SUBCELLULAR LOCATION: Mitochondrion {ECO:00002...      240   5146   \n",
       "11492  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...      238  11492   \n",
       "1594   SUBCELLULAR LOCATION: Cytoplasm, cytosol {ECO:...      325   1594   \n",
       "\n",
       "       label  \n",
       "3619       1  \n",
       "907        1  \n",
       "5146       0  \n",
       "11492      1  \n",
       "1594       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_url =\"https://rest.uniprot.org/uniprotkb/stream?compressed=true&fields=accession%2Csequence%2Ccc_subcellular_location&format=tsv&query=%28%28organism_id%3A9606%29%20AND%20%28reviewed%3Atrue%29%20AND%20%28length%3A%5B80%20TO%20500%5D%29%29\"\n",
    "uniprot_request = requests.get(query_url)\n",
    "bio = BytesIO(uniprot_request.content)\n",
    "df = pandas.read_csv(bio, compression='gzip', sep='\\t')\n",
    "df['seq_len'] = list(map(len, df.Sequence))\n",
    "df = df.dropna()\n",
    "df.sort_values('seq_len', ascending = False)\n",
    "df['ind'] = list(df.index)\n",
    "cytosolic = df['Subcellular location [CC]'].str.contains(\"Cytosol\") | df['Subcellular location [CC]'].str.contains(\"Cytoplasm\")\n",
    "membrane = df['Subcellular location [CC]'].str.contains(\"Membrane\") | df['Subcellular location [CC]'].str.contains(\"Cell membrane\")\n",
    "cytosolic_df = df[cytosolic & ~membrane]\n",
    "cytosolic_df['label'] = 0\n",
    "membrane_df = df[membrane & ~cytosolic]\n",
    "membrane_df['label'] = 1\n",
    "df = pd.concat([cytosolic_df, membrane_df]).sort_values('ind').sample(frac = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5115cf64-4047-4075-838a-8995aa32ffdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5149"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40749b-5c91-418b-93cd-390f2238604e",
   "metadata": {},
   "source": [
    "Let's try passing a sequence through the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e0753-9087-45c6-93ec-0ab585a73ec7",
   "metadata": {},
   "source": [
    "# PEFT using GaLoRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7e1ea8-1032-47db-805f-0e09509769c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Sequence','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b755280-5b70-4e41-a087-1485a3a2d6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    2599\n",
       "1    2550\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3602f687-6ade-41b6-9121-48ae759e3168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suhas/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from transformers import TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "# import trl\n",
    "\n",
    "target_modules = []\n",
    "for layer in range(6):\n",
    "    for elem in ['query','key','value']:\n",
    "        target_modules.append(\"esm.encoder.layer.\"+str(layer)+\".attention.self.\" + elem)\n",
    "model_name = model_checkpoint.split('/')[-1]\n",
    "batch_size = 8\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-lora-finetuned-localization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    optim=\"galore_adamw\",\n",
    "    optim_target_modules=target_modules\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8704a7c-facc-4c61-99f7-7fd3bf513d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels = len(set(df.label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847a916-1bf7-41c7-bb31-433565c304fd",
   "metadata": {},
   "source": [
    "Let us create a training and test dataset from df, and also let us tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9bf2959-177d-4598-b9f0-056ef4a43be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5149, Index(['Sequence', 'label'], dtype='object'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "292c9ae9-7d75-46a8-898f-fa6b3878ce1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = list(df.Sequence)\n",
    "labels = list(df.label)\n",
    "\n",
    "# Quick check to make sure we got it right\n",
    "len(sequences) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dec8edaa-9bcb-4340-9bc4-b0b2e5a89ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffeb17a7-12e2-4e93-8df5-83b53d23fbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list, list, list)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_sequences), type(test_sequences), type(train_labels), type(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7283a3f0-8ee9-42cd-8877-ea384dab70d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3861, 1288, 3861, 1288)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequences), len(test_sequences), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d52ddf06-a3ff-49da-9ec4-39e83e3ce898",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9d9b098-64ae-43ef-aef1-5a2875eb0815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.tokenization_utils_base.BatchEncoding, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tokenizer(train_sequences[0])\n",
    "type(z), len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbdeab45-d001-4390-8071-eac82a97c606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2b94997-e11e-48b2-910e-8e5bbb7e9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9990b715-045d-4d25-be73-e564fc86441e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 3861\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 1288\n",
       " }))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a736fe0-c0b8-4091-a6e7-1e342deff1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'esm2_t6_8M_UR50D'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[1]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "356e62ec-5a95-4816-88ab-ed7c446c3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95d5b6bd-e6a2-4661-b1e0-7b6cb21ff4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suhas/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e7801-f558-474c-a0c7-5cdbf6a0eaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Activated GaLoRE fine-tuning, depending on your model size and hardware, the training might take a while before starting. Please be patient !\n",
      "/home/suhas/.local/lib/python3.10/site-packages/galore_torch/adamw.py:48: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92' max='1449' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  92/1449 04:06 < 1:02:02, 0.36 it/s, Epoch 0.19/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c5e23-6835-4252-952d-c1cb7cffb589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
